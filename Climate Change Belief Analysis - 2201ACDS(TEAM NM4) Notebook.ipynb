{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c8737a",
   "metadata": {},
   "source": [
    "# <font style='color: firebrick; text-align:center; display:block'> Climate Change Belief Analysis </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4581c7",
   "metadata": {},
   "source": [
    "By:\n",
    "1. Patrick Onduto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4aae42",
   "metadata": {},
   "source": [
    "## <font color='goldenrod'>Table of Contents</font>\n",
    "[TOC]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376b935",
   "metadata": {},
   "source": [
    "\n",
    "## <font color='goldenrod'>1. Introduction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0155ea",
   "metadata": {},
   "source": [
    "### 1.1. Problem Description\n",
    "Today, many companies are built around lessening oneâ€™s environmental impact or carbon footprint. They are offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. A majority of these companies would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, we are creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution will give companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cc335",
   "metadata": {},
   "source": [
    "### 1.2. Data Description\n",
    "The data used to build the Machine models was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo.\n",
    "The dataset contains 43943 aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018 with each tweet is labelled as one of the following classes:\n",
    "\n",
    "**Class Description**\n",
    " \n",
    "- 2 *``News``*: the tweet links to factual news about climate change\n",
    "- 1 *``Pro``*: the tweet supports the belief of man-made climate change\n",
    "- 0 *``Neutral``*: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "- -1 *``Anti``*: the tweet does not believe in man-made climate change\n",
    "\n",
    "**Variable definitions**\n",
    "- **sentiment**: Sentiment of tweet\n",
    "- **message**: Tweet body\n",
    "- **tweetid**: Twitter unique id\n",
    "\n",
    "For more on the data description [view source here](https://www.kaggle.com/competitions/edsa-climate-change-belief-analysis-2022/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422512c5",
   "metadata": {},
   "source": [
    "## <font color='goldenrod'>2. Importing Packages</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699627a",
   "metadata": {},
   "source": [
    "Let's load python package that will be use to load the datasets, clean and format it, analysis and visualize it, and build, evaluate and draw the Machine Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eac88b",
   "metadata": {
    "id": "b9eac88b"
   },
   "outputs": [],
   "source": [
    "import warnings # to filter out warnings in the jupyter notebook\n",
    "warnings.filterwarnings('ignore') # we will ignore all warning and not show them\n",
    "\n",
    "import re # for creating regex expressions                               \n",
    "import string # for manipulating strings\n",
    "\n",
    "# Imports to load, explore, wrangle and visualize\n",
    "import numpy as np # for data manipulation\n",
    "import pandas as pd # for loading and data manipulation\n",
    "import matplotlib.pyplot as plt # for drawing visualizations\n",
    "import seaborn as sns # for drawing visualizations\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports for data preprocessing\n",
    "from nltk.corpus import stopwords # for generatig english stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # for word stem/lemma production \n",
    "from nltk.tokenize import TweetTokenizer, TreebankWordTokenizer # for tokenizing text to words\n",
    "from sklearn.utils import resample # for resampling our dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer # for vectorizing and transforming the data\n",
    "from sklearn.model_selection import train_test_split # for spliting data to training and testing sets\n",
    "\n",
    "# Imports for model building\n",
    "from sklearn.linear_model import LogisticRegression # for making a Logistic Regression classifier\n",
    "from sklearn.svm import SVC  # for making a Support Vector Machine classifier\n",
    "from sklearn.naive_bayes import MultinomialNB # for making a Naive Bayes classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # for making a Random Forest classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  # for making a K Neighbors classifier\n",
    "from sklearn.model_selection import GridSearchCV # for cross validation of models\n",
    "\n",
    "# Imports for model evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score # for scoring our models\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix # for creating and drawing a confusion matrix respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2852a7",
   "metadata": {},
   "source": [
    "## <font color='goldenrod'>3. Loading Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83199512",
   "metadata": {},
   "source": [
    "Let's load the *training dataset*. This dataset will allow us to predict the sentiment of a person based on their tweet text.\n",
    "We will use pandas' (i.e. pd) dataframe to load and view as table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a654e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "4b1a654e",
    "outputId": "d817f820-bdc0-494f-c5f0-6354d45e5c09"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(# load the dataset as dataframe, set tweetid as index column\n",
    "    './data/train.csv', index_col='tweetid', encoding='utf-8'\n",
    ") \n",
    "df.head() # view the first five row of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f48fcb",
   "metadata": {},
   "source": [
    "After loading our data, let's view the shape of our dataframe to have a sense how the dataframe is constituted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5439343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c93200",
   "metadata": {},
   "source": [
    "Now that we have loaded and viewing the data as a dataframe, we can assertain from the first five rows of the dataframe that: \n",
    "- we have 3 columns in our dataset: ``tweetid``, ``sentiment`` and ``message`` column.\n",
    "- We make the ``tweetid`` column the index column of the dataframe as it has unique values.\n",
    "\n",
    "Also, the dataframe has *15819* rows (tweet observations) that we will be using to train and test our models with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a749289",
   "metadata": {
    "id": "5a749289"
   },
   "source": [
    "## <font color='goldenrod'>4. Exploratory Data Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7328a06",
   "metadata": {},
   "source": [
    "### 4.1 Identifying columns data type and and missing values\n",
    "Let's first make sure that our data is correctly formated before analysing and using it. We will investigate the data type of each of the columns and also make sure that we have no missing values in our data and deal with any problems if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c15a83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0c15a83",
    "outputId": "4df0c26a-4977-431a-de24-9490c4fe96da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see if there are any missing values\n",
    "df.isna().sum().to_frame(name='Total missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c08769",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25c08769",
    "outputId": "96022e43-efd8-428e-c555-f507714f0fe9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view the data type of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c946672",
   "metadata": {},
   "source": [
    "After investigating the missing values and columns data types, we can conclude that our data is clean and thus does not need any further formating or imputation.\n",
    "The ``sentiment`` column of datatype ``int``, correct for Machine Learning models.\n",
    "Since this a Natural Language Processing Machine Learning problem, we will creating transforming our ``message`` column (of datatype ``object``) to create our features. (**NB:** This will be accomplished in the [Data preprocessing](##data_preprocessing) part.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd213ec",
   "metadata": {},
   "source": [
    "### 4.2. Tweets distribution\n",
    "Next, we will investigate the amount of tweet messages allocated to each of the sentiment categories, giving a snapshot of our data distribution and highlighting any data inbalance in the dataset. This will ultimately inform if we need to employ to resampling technique to either upsample, downsample or both, to balance our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56292172",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "56292172",
    "outputId": "88fa83db-f322-4dc8-eb2d-48a2f1284522",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create table to shows the sentiment types and the total number of tweets identified to each of them\n",
    "# Let's group dataframe by the sentiment column and aggregate our messages by counting them by sentiment category\n",
    "# sort by 'message counts' descending and then extract the 'message' column \n",
    "total_messages_per_sentiment_type = df.groupby(\n",
    "    'sentiment').count().sort_values('message', ascending=False)['message']\n",
    "# lastly, reset the index rename 'message' column to 'message counts'\n",
    "total_messages_per_sentiment_type.reset_index(\n",
    "    name='message counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31205b3",
   "metadata": {},
   "source": [
    "Let's represent this information in a count plot to see how they are graphically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e40928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, let's visually view number of message per sentiment type\n",
    "ax = sns.countplot(\n",
    "    x=\"sentiment\",\n",
    "    data=df,\n",
    "    order=total_messages_per_sentiment_type.index,\n",
    "    palette='Oranges_r') # draw a countplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19284ce4",
   "metadata": {},
   "source": [
    "Let's can also plot a donut chart to know view percentage of tweets fall under each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A donut chart that shows the total percentage of tweet for each sentiment group\n",
    "labels = total_messages_per_sentiment_type.index\n",
    "plt.figure(figsize=(6, 6))\n",
    "# pie chart\n",
    "plt.pie(total_messages_per_sentiment_type, labels=labels, autopct='%1.1f%%',\n",
    "    colors = sns.color_palette(palette='Oranges_r', n_colors=4), pctdistance=0.85,\n",
    "    explode = (0.01, 0.01, 0.01, 0.01), textprops={\"fontsize\":14}\n",
    ")\n",
    "\n",
    "centre_circle = plt.Circle((0, 0), 0.50, fc='white') # draw circle\n",
    "fig = plt.gcf()\n",
    "  \n",
    "fig.gca().add_artist(centre_circle) # add a circle in pie chart\n",
    "plt.title('% Proportion of Messages by Sentiment',  fontdict={\"fontsize\":14}) # add a title of chart\n",
    "  \n",
    "plt.show() # displaying chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897d3f0",
   "metadata": {},
   "source": [
    "This is quite interesting! We can see that the distribution of the ``sentiment`` types are highly imbalanced with:\n",
    "- ``Pro`` (1) sentiment has a total of 8530 messages, the largest portion\n",
    "-  ``News`` (2) sentiment has a total of 3640 messagess\n",
    "-  ``Neutral`` (0) sentiment has a total of 2353 messages\n",
    "- ``Anti`` (-1) sentiment has a total of 1296 messages, the smallest portion\n",
    "\n",
    "This implies that we need to resample to be able to achieve a balanced data set for training our models. This will make sure that the model predictions are not biased to *sentiments* with large portions of *messages*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c04f18",
   "metadata": {},
   "source": [
    "### 4.3. Words distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0451e3e",
   "metadata": {},
   "source": [
    "Now to understand more about our messages, a distribution of the total number off words in each of our tweets. We will plot a histogram for this distribution for each of the sentiment categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f45f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dist = df.copy() # Make copy of dataframe\n",
    "# Split the tweet into a list of words and count the number of words in the text\n",
    "tweet_dist['words'] = tweet_dist['message'].apply(\n",
    "    lambda text: len(text.split())\n",
    ") \n",
    "\n",
    "g = sns.FacetGrid(# Create a grid to visualize the sentiment\n",
    "    tweet_dist, col = 'sentiment', col_wrap=2, hue='sentiment', size=4) \n",
    "g.fig.subplots_adjust(top=0.9) # adjust the Figure in grid\n",
    "g.fig.suptitle('Distribution of Words per Message') # add title to the grid\n",
    "g.map(sns.histplot, 'words')  # plot a histogram to visualize the ditribution of words for each of the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aecab6",
   "metadata": {
    "id": "60aecab6"
   },
   "source": [
    "## <font color='goldenrod'>5. Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a487e3",
   "metadata": {
    "id": "c2a487e3"
   },
   "source": [
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm.\n",
    "Also, to remove learning bias from our models, we need to balance our data when we have an inbalanced classes in our dataset.\n",
    "For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "- Tokenizing the string\n",
    "- Lowercasing\n",
    "- Removing stop words and punctuation\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0ebd3",
   "metadata": {
    "id": "3de0ebd3"
   },
   "source": [
    "#### 5.1 Dealing with Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ad86e",
   "metadata": {},
   "source": [
    "Since we have imbalanced data for our classes and we need the models to predict all the classes without any issue, we need to consider balancing it. While a slight imbalance wouldnâ€™t be a problem, a highly imbalanced dataset like ours can cause issues to the classification predictions. And as most machine learning algorithms rely on sufficient data, the algorithm canâ€™t correctly predict its result when some of the classes have little data.\n",
    "\n",
    "Now, let's balance our data. With the ``Pro`` (1) has the largest proportion of **8530** we use that to set our sample size for each of the sentiment classes, say roughly about half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9O956amjy7SK",
   "metadata": {
    "id": "9O956amjy7SK"
   },
   "outputs": [],
   "source": [
    "# Let's pick a class size of roughly half the size of the largest size\n",
    "class_size = 5000\n",
    "# Let's list the target labels\n",
    "labels_counts = df['sentiment'].value_counts().to_dict() # create a dict with the setiment  as keys and counts as values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8862f",
   "metadata": {},
   "source": [
    "We, need to have now downsample the ``Pro`` (1) sentiment class while we upsample all the other sentiment classes by setting the ``replace`` attribute of skit learn's ``resample`` method to ``False`` and ``True`` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LpJXtTSdy-yT",
   "metadata": {
    "id": "LpJXtTSdy-yT"
   },
   "outputs": [],
   "source": [
    "resampled_classes = []\n",
    "\n",
    "# For each label\n",
    "for label, label_size in labels_counts.items():\n",
    "    # If label_size < class size then set replace to True to upsample, else False to downsample\n",
    "    if label_size < class_size:\n",
    "        # Upsample\n",
    "        replacement = True # sample with replacement (we need to duplicate observations)\n",
    "    else:\n",
    "        # Downsample\n",
    "        replacement = False # sample with replacement (we need to duplicate observations)\n",
    "    label_data = df[df['sentiment'] == label]\n",
    "    label_resampled = resample(label_data,\n",
    "                               replace=replacement, # sample without replacement (no need for duplicate observations)\n",
    "                               n_samples=class_size, # number of desired samples\n",
    "                               random_state=27) # reproducible results\n",
    "\n",
    "    resampled_classes.append(label_resampled) # add to the resampled_classes list the sampled dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff74b56",
   "metadata": {},
   "source": [
    "Finally, let's concatenate all the resample dataframes to have a final balanced dataset to train our models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70qXELn2tnD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d70qXELn2tnD",
    "outputId": "ffcfcb13-9a6a-4678-b7ef-552cf3a4c61d"
   },
   "outputs": [],
   "source": [
    "resampled_data = pd.concat(resampled_classes, axis=0) # add the dataframes for the list together to have a final dataframe\n",
    "print(resampled_data.shape) # view the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b577fb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "2b577fb2",
    "outputId": "3262fcb3-25e7-4c8d-db8e-2baa697d3598",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw a bar graph to now show the final distribution classes of the resampled dataframe\n",
    "ax = sns.countplot(x=\"sentiment\", data=resampled_data, color='Orange', order=[1, 2, 0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19b312",
   "metadata": {},
   "source": [
    "Our datase now contains 20,000 tweets in total distributed 5,000 tweets four way making our classses have a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8fd67",
   "metadata": {
    "id": "27e8fd67"
   },
   "source": [
    "#### 5.2 Remove hyperlinks, Twitter marks, and emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28964d2d",
   "metadata": {},
   "source": [
    "Let's explore some of tweets had see how they are composed to understand on how to now process the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetids_to_explore = df.loc[[625221, 698562, 573736, 911385, 483815], 'message'] # select a sample of the tweets\n",
    "for m in tweetids_to_explore:\n",
    "    print(m, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41207654",
   "metadata": {
    "id": "41207654"
   },
   "source": [
    "From the five tweets we sampled, we can observe the following about our tweet messages.\n",
    "\n",
    "- the beginning tweet **RT** that indicates one's re-posting someone else's content\n",
    "- the official **@** and **#** character signifying handles and trends respectively\n",
    "- the punctuations marks, emoticons, urls etc.\n",
    "\n",
    "We need to do a lot of preprocessing to work have good clean data to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d7217",
   "metadata": {},
   "source": [
    "First, let's creat a function to strip the emoticons from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2749139",
   "metadata": {
    "id": "f2749139"
   },
   "outputs": [],
   "source": [
    "# function to strip emojis from the tweets\n",
    "#Ref: https://www.kaggle.com/code/eliasdabbas/how-to-create-a-python-regex-to-extract-emoji/notebook\n",
    "# Ref: https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1\n",
    "# Ref: https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1#gistcomment-3208085\n",
    "# Ref: https://en.wikipedia.org/wiki/Unicode_block\n",
    "\n",
    "def remove_emoji(text):\n",
    "    EMOJI_PATTERN = re.compile(\n",
    "        \"([\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"])\"\n",
    "    )\n",
    "    text = re.sub(EMOJI_PATTERN,  '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79aba96",
   "metadata": {},
   "source": [
    "Now, we need to create a function that actually handles the processing a tweet message, removing all emoticons, punctuation marks, tweet handles and trends, and removes english stopword.\n",
    "We then create lemmas of the remaining words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7ef0d",
   "metadata": {
    "id": "d6f7ef0d"
   },
   "outputs": [],
   "source": [
    "def tweet_preprocessor(tweet):\n",
    "    # remove the old style retweet text \"RT\"\n",
    "    tweet_clean = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hashtags. We have to be careful here not to remove \n",
    "    # the whole hashtag because text of hashtags contains huge information. \n",
    "    # only remove the hash # sign from the word\n",
    "    tweet_clean = re.sub(r'#', '', tweet_clean)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet_clean = re.sub(\n",
    "        r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "        ,r'url-web',\n",
    "        tweet_clean)\n",
    "\n",
    "    # remove single numeric terms in the tweet. \n",
    "    tweet_clean = re.sub(r'\\s[0-9]+\\s', '', tweet_clean)\n",
    "\n",
    "    # remove emojis from in the tweet\n",
    "    tweet_clean = remove_emoji(tweet_clean)\n",
    "\n",
    "    # remove punctuation from the tweet\n",
    "    tweet_clean = ''.join([l for l in tweet_clean if l not in string.punctuation])\n",
    "\n",
    "    # convert tweet to lowercase and return it\n",
    "    tweet_clean = tweet_clean.lower()\n",
    "    \n",
    "    # tokenize the tweet\n",
    "    tokenizer = TweetTokenizer() #Instantiate the tokenizer class\n",
    "    tweet_tokens = tokenizer.tokenize(tweet_clean)\n",
    "    \n",
    "    # remove stop words\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet_tokens_without_stopwords = [t for t in tweet_tokens if t not in stopwords_english]\n",
    "    \n",
    "    # stem the tweet\n",
    "    #stemmer = PorterStemmer()\n",
    "    #tweet_stems = ' '.join([stemmer.stem(t) for t in tweet_tokens])\n",
    "    \n",
    "    # stem the tweet\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tweet_stems = ' '.join([lemma.lemmatize(t) for t in tweet_tokens])\n",
    "    \n",
    "    return tweet_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a4c1f",
   "metadata": {},
   "source": [
    " After creating our functions, we will need to to split the dataset into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede02688",
   "metadata": {
    "id": "ede02688"
   },
   "outputs": [],
   "source": [
    "#Get the features and targets\n",
    "X = resampled_data['message']\n",
    "y = resampled_data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6bce88",
   "metadata": {},
   "source": [
    "The text documents in their original form can not be processed by the model classifiers and learning algorithms as most of them expect numerical feature vectors.\n",
    "\n",
    "Rather than the raw text documents that have variable length, we will convert the ``message`` column to a bag of words vector using the skit learn's ``CountVectorizer`` to produce our features.\n",
    "We will, specifically, calculate a measure called Term Frequency, Inverse Document Frequency, abbreviated to tf-idf, for each term in the vectorized features using skit learn's ``TfidfTransformer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48d642",
   "metadata": {
    "id": "fa48d642"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer( # Instantiate the object\n",
    "    preprocessor=tweet_preprocessor,\n",
    "    min_df=2, # with min_df of 2 words in our bag of words\n",
    "    max_df=0.5, # with max_df of 50% word frequency in our bag of words\n",
    "    ngram_range=(1,2) # with ngram_range of (1,2) for bag of words\n",
    ") \n",
    "vectorizer.fit(X) #build vocabulary for training\n",
    "X_tokenized = vectorizer.transform(X) #encode the text data \n",
    "\n",
    "# scale the encoded text data \n",
    "tfidf_transformer = TfidfTransformer() # Instantiate the object\n",
    "X_trans = tfidf_transformer.fit_transform(X_tokenized) # Transform the tokens\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8b268",
   "metadata": {},
   "source": [
    "Now, each of the *20000* tweet messages is represented by *50602* features, representing the tf-idf score for different unigrams and bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40456f17",
   "metadata": {
    "id": "40456f17"
   },
   "source": [
    "## <font color='goldenrod'>6. Building the Models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56d7a5",
   "metadata": {},
   "source": [
    "Now it is time to build our models.\n",
    "\n",
    "For our of problem we are going to experiment with five different algorithms and explore their performance to find the performing best model.\n",
    "\n",
    "Before we build the models, we are going to first split our features and labels into training and testing sets, create a list object containing our benchmark models and the hyper parameters that we will use to tune them (later), and lastly, we create functions to fit and evaluate the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abb8a1",
   "metadata": {},
   "source": [
    "Let's then split the features and labels to training and testing sets to have our ``X_train``, ``y_train`` , ``X_test`` and ``y_test``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60014960",
   "metadata": {
    "id": "60014960"
   },
   "outputs": [],
   "source": [
    "#Split the labels and target into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e396f",
   "metadata": {},
   "source": [
    "Then instatiate our models in a list with the hyperparameters need for tuning them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list containing the various models to evaluate\n",
    "# include the parameters we are interested to tune for each model\n",
    "models = [\n",
    "    ## define the model Support Vector Classifier\n",
    "    {  \n",
    "        'model': SVC(), # instantiate the model\n",
    "        'param_grid': { # set hyper parameters for model tuning\n",
    "            'C': [1, 10, 100],\n",
    "            'gamma': [0.1,0.01,0.001],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "    },\n",
    "    ## define the model Random Forest Classifier\n",
    "    {  \n",
    "        'model': RandomForestClassifier(), # instantiate the model\n",
    "        'param_grid': { # set hyper parameters for model tuning\n",
    "            'max_depth': [10, 20, 40, 50, 100, None],\n",
    "            'n_estimators': [200, 400, 600, 800, 1000]\n",
    "        }\n",
    "    },\n",
    "    ## define the model Logistic Regression Classifier\n",
    "    {  \n",
    "        'model': LogisticRegression(), # instantiate the model\n",
    "        'param_grid': { # set hyper parameters for model tuning\n",
    "            'C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'solver' : ['lbfgs', 'liblinear'],\n",
    "            'max_iter' : [100, 1000, 2500]\n",
    "        }\n",
    "    },\n",
    "    ## define the model KNeighbors Classifier\n",
    "    { \n",
    "        'model': KNeighborsClassifier(), # instantiate the model\n",
    "        'param_grid': { # set hyper parameters for model tuning\n",
    "            'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        }\n",
    "    },\n",
    "    ## define the mode MultinomialNB Classifier\n",
    "    {\n",
    "        'model': MultinomialNB(), # instantiate the model\n",
    "        'param_grid': { # set hyper parameters for model tuning\n",
    "            'alpha': [0.0001, 0.001, 0.1, 1, 10]\n",
    "        }\n",
    "    }    \n",
    "]      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d634f",
   "metadata": {},
   "source": [
    "Finally, let's create functions to fit the models and evaluate them..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d39f9",
   "metadata": {},
   "source": [
    "###### 1. function to fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d940f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit the base models for on the data\n",
    "# to view their initial performance\n",
    "def fit_models(X_train, y_train, models):\n",
    "    models_list = []\n",
    "    for item in models:\n",
    "        model = item['model']\n",
    "        model.fit(X_train, y_train)\n",
    "        models_list.append((model.__class__.__name__, model))\n",
    "    return models_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf2efa",
   "metadata": {},
   "source": [
    "###### 2. function to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff9d9e",
   "metadata": {
    "id": "abff9d9e"
   },
   "outputs": [],
   "source": [
    "# Function to predict targets for X_test\n",
    "# and to return the accuracy and f1 scores, classification report, and confusion matrix\n",
    "def evaluate_models(X_test, y_test, models):\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    classification_reports = []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for (name, fitted_model) in fitted_models:\n",
    "        y_pred = fitted_model.predict(X_test)\n",
    "        accuracy_scores.append( # add to accuracy_scores the accuracy of model\n",
    "            accuracy_score(y_test, y_pred) # find the accuracy\n",
    "        )\n",
    "        f1_scores.append( # add to f1_scores the f1 score of model\n",
    "            f1_score(y_test, y_pred, average='weighted') # find the f1 score\n",
    "        )\n",
    "        classification_reports.append( # add to f1_scores the f1 score of model\n",
    "            (name.upper(), classification_report(y_test, y_pred))\n",
    "        )\n",
    "        confusion_matrices.append(\n",
    "            (name.upper(), confusion_matrix(y_test, y_pred))\n",
    "        )\n",
    "    return (accuracy_scores, f1_scores), (classification_reports, confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56706b20",
   "metadata": {},
   "source": [
    "Finally, having finished creating all those, let's proceed to training our models on the training data sets (``X_train``,``y_train``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8643cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the various model\n",
    "fitted_models = fit_models(X_train, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9558890",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models # View the trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a266d3",
   "metadata": {},
   "source": [
    "With our benchmark models trained, we need to evaluate their performance. We will use th skit learn's ``f1`` and ``accuracy`` scores to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f690f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we predict the target for the X_test for the models\n",
    "# and get the accuracy and f1 scores\n",
    "(accuracy_scores, f1_scores), _ = evaluate_models(X_test, y_test, fitted_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f6d38",
   "metadata": {},
   "source": [
    "Let's view the scores of the models on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tP_YaPtS_tMi",
   "metadata": {
    "id": "tP_YaPtS_tMi"
   },
   "outputs": [],
   "source": [
    "# let's create a Dataframe to tabulate the performance of various models\n",
    "base_models_performance_df = pd.DataFrame(\n",
    "    zip([i[0] for i in fitted_models], accuracy_scores, f1_scores),\n",
    "    columns = ['model', 'accuracy', 'f1 score']\n",
    ")\n",
    "# and view the output as a table\n",
    "base_models_performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a422f",
   "metadata": {},
   "source": [
    "From the default models, we can observe that the SVC and Random Forest are the best (with the SVC having a slight edge), while alsothe Multinomial and Logistic coming in close. Only the Kneighbours has score that is far lower that the other four models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b03e0",
   "metadata": {
    "id": "253b03e0"
   },
   "source": [
    "## <font color='goldenrod'>7.  Tuning the Hyper Parameter of Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d28d05",
   "metadata": {},
   "source": [
    "To finish the selection of our **best model**, we need to tune the models with the selected hyper parameter to see if we can get the best performing model. From these models we will then select the best scoring model with the best hyper parameters, using ``f1`` score.\n",
    "\n",
    "To do this we will create a function that we will use to tune the models, before finally evaluate their performance on ``f1`` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e164c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tune our base models\n",
    "# and find the optimal params for each of our model \n",
    "# We use the f1 score as our scoring criteria\n",
    "# and a cross-validation splitting (cv) of 10\n",
    "\n",
    "def tuned_models(X_train, y_train, models):\n",
    "    tuned_models = [] # initialise an empty list\n",
    "    for item in models:\n",
    "        model = item['model'] # get the model object instance\n",
    "        param_grid = item['param_grid'] # get the parameters that will be used for tuning\n",
    "        # define the grid instance\n",
    "        grid =  GridSearchCV(\n",
    "            model, # model instance\n",
    "            param_grid = param_grid, # hyper-parameters\n",
    "            cv = 10, # cross-validation splitting\n",
    "            scoring = 'f1_weighted', # f1_scoring\n",
    "            refit=True\n",
    "        )\n",
    "        print(f'Fitting {model.__class__.__name__} model....')\n",
    "        grid.fit(X_train, y_train) # fit the grid to the X-train and the y_train\n",
    "        tuned_models.append({ # add to tuned_models list the\n",
    "            'name': model.__class__.__name__, # model name\n",
    "            'tuned model': grid, # best model instance\n",
    "            'best params': grid.best_params_, # best model parameters\n",
    "            'f1 score': grid.best_score_, # best f1 score\n",
    "            'refit time': grid.refit_time_ # the time the model took to refit the data\n",
    "        })\n",
    "        print(f'Finished fitting {grid.best_estimator_}')\n",
    "        \n",
    "    return tuned_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885bc3d",
   "metadata": {},
   "source": [
    "Let's now tune our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf00a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the models' parameters to produce the best model estimators\n",
    "tuned_models = tuned_models(X_train, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lmdICT6EfkDA",
   "metadata": {
    "id": "lmdICT6EfkDA"
   },
   "outputs": [],
   "source": [
    "# let's create a Dataframe to tabulate best estimators and their performance\n",
    "tuned_models_performance_df = pd.DataFrame(\n",
    "    [(model['name'], model['best params'], model['f1 score'], model['refit time']) for model in tuned_models],\n",
    "    columns = ['Model', 'Best Params', 'f1 Score', 'Refit Time'])\n",
    "# and view the output as a table\n",
    "tuned_models_performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f37e1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29bf9df0",
   "metadata": {
    "id": "29bf9df0"
   },
   "source": [
    "## <font color='goldenrod'>8. Evaluating our Model Performance</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370a001",
   "metadata": {},
   "source": [
    "For the tuned models, let's view the f1 score of each tuned model. We then will selected the model with the overall best *f1 score* as our model and as evaluate its classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a019d5",
   "metadata": {},
   "source": [
    "Continue with our best model -------, we are going to look at the confusion matrix, and show the discrepancies between predicted and actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19858a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "19858a93",
    "outputId": "bbe8b4a0-80ae-45f1-c521-b66b79cf9369"
   },
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred_log)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Oranges')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed36d14",
   "metadata": {
    "id": "fed36d14"
   },
   "source": [
    "## KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f76ba6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "c9f76ba6",
    "outputId": "0401d812-52da-4c2e-9d1b-e9cbd6dee93f"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test_with_no_labels.csv', index_col='tweetid')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be979a",
   "metadata": {
    "id": "11be979a"
   },
   "outputs": [],
   "source": [
    "test_tokenized = vectorizer.transform(test_df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2T0w0WAXOwMW",
   "metadata": {
    "id": "2T0w0WAXOwMW"
   },
   "outputs": [],
   "source": [
    "test_trans = tfidf_transformer.fit_transform(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (fitted_name, fitted_model) in fitted_models:\n",
    "    test_pred = fitted_model.predict(test_trans)\n",
    "    results_df = pd.DataFrame(test_pred.tolist(), index = test_df.index, columns=['sentiment'])\n",
    "    results_df.to_csv(f\"{fitted_name}_classification_predict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db008038",
   "metadata": {
    "id": "db008038"
   },
   "outputs": [],
   "source": [
    "for tuned_model in tuned_models:\n",
    "    model_instance = tuned_model['tuned model']\n",
    "    test_pred = model_instance.predict(test_trans)\n",
    "    results_df = pd.DataFrame(test_pred.tolist(), index = test_df.index, columns=['sentiment'])\n",
    "    results_df.to_csv(f\"{tuned_model['name']}_classification_predict.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Starter_Notebook.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
